from __future__ import annotations
import os
import re
from datetime import datetime, timezone
from zoneinfo import ZoneInfo

from .fetch import fetch_news
from .extract import load_known_cases, build_lawsuits_from_news
from .render import render_markdown
from .github_issue import find_or_create_issue, create_comment, close_other_daily_issues
from .github_issue import list_comments
from .slack import post_to_slack
from .utils import debug_log, slugify_case_name
from .dedup import apply_deduplication
from .courtlistener import (
    search_recent_documents,
    build_complaint_documents_from_hits,
    build_case_summaries_from_hits,
    build_case_summaries_from_docket_numbers,
    build_case_summaries_from_case_titles,
    build_documents_from_docket_ids,
)
from .queries import COURTLISTENER_QUERIES

def main() -> None:
    # 0) í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    owner = os.environ.get("GITHUB_OWNER")
    repo = os.environ.get("GITHUB_REPO")
    gh_token = os.environ.get("GITHUB_TOKEN")
    slack_webhook = os.environ.get("SLACK_WEBHOOK_URL")

    if not all([owner, repo, gh_token, slack_webhook]):
        missing = [k for k, v in {"GITHUB_OWNER": owner, "GITHUB_REPO": repo, "GITHUB_TOKEN": gh_token, "SLACK_WEBHOOK_URL": slack_webhook}.items() if not v]
        raise ValueError(f"í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing)}")

    base_title = os.environ.get("ISSUE_TITLE_BASE", "AI ë¶ˆë²•/ë¬´ë‹¨ í•™ìŠµë°ì´í„° ì†Œì†¡ ëª¨ë‹ˆí„°ë§")
    lookback_days = int(os.environ.get("LOOKBACK_DAYS", "3"))
    # í•„ìš” ì‹œ 2ë¡œ ë³€ê²½: í™˜ê²½ë³€ìˆ˜ LOOKBACK_DAYS=2
    
    # KST ê¸°ì¤€ ë‚ ì§œ ìƒì„±
    now_kst = datetime.now(ZoneInfo("Asia/Seoul"))
    run_ts_kst = now_kst.strftime("%Y-%m-%d %H:%M")
    issue_day_kst = now_kst.strftime("%Y-%m-%d")
    issue_title = f"{base_title} ({issue_day_kst})"
    debug_log(f"KST ê¸°ì¤€ ì‹¤í–‰ì‹œê°: {run_ts_kst}")
    
    issue_label = os.environ.get("ISSUE_LABEL", "ai-lawsuit-monitor")

    # 1) CourtListener ê²€ìƒ‰
    hits = []
    for q in COURTLISTENER_QUERIES:
        debug_log(f"Running CourtListener query: {q}")
        hits.extend(search_recent_documents(q, days=lookback_days, max_results=20))
    
    # ì¤‘ë³µ ì œê±°
    dedup = {}
    for h in hits:
        key = (h.get("absolute_url") or h.get("url") or "") + "|" + (h.get("caseName") or h.get("title") or "")
        dedup[key] = h
    hits = list(dedup.values())

    cl_docs = build_complaint_documents_from_hits(hits, days=lookback_days)
    # RECAP ë„ì¼“(ì‚¬ê±´) ìš”ì•½: "ë²•ì› ì‚¬ê±´(ë„ì¼“) í™•ì¸ ê±´ìˆ˜"ë¡œ ì‚¬ìš©
    cl_cases = build_case_summaries_from_hits(hits)

    # 2) ë‰´ìŠ¤ ìˆ˜ì§‘
    news = fetch_news()
    known = load_known_cases()
    lawsuits = build_lawsuits_from_news(news, known, lookback_days=lookback_days)

    # 2-1) ë‰´ìŠ¤ í…Œì´ë¸”ì˜ ì†Œì†¡ë²ˆí˜¸(ë„ì¼“ë²ˆí˜¸)ë¡œ RECAP ë„ì¼“/ë¬¸ì„œ í™•ì¥
    docket_numbers = [s.case_number for s in lawsuits if (s.case_number or "").strip() and s.case_number != "ë¯¸í™•ì¸"]
    extra_cases = build_case_summaries_from_docket_numbers(docket_numbers)

    # 2-2) ì†Œì†¡ë²ˆí˜¸ê°€ ì—†ë”ë¼ë„, 'ì†Œì†¡ì œëª©'(ì¶”ì • ì¼€ì´ìŠ¤ëª…)ìœ¼ë¡œ ë„ì¼“ í™•ì¥
    case_titles = [s.case_title for s in lawsuits if (s.case_title or "").strip() and s.case_title != "ë¯¸í™•ì¸"]
    extra_cases_by_title = build_case_summaries_from_case_titles(case_titles)

    merged_cases = {c.docket_id: c for c in (cl_cases + extra_cases + extra_cases_by_title)}
    cl_cases = list(merged_cases.values())

    # ë¬¸ì„œë„ docket id ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ ì‹œë„(Complaint ìš°ì„ , ì—†ìœ¼ë©´ fallback)
    docket_ids = list(merged_cases.keys())
    extra_docs = build_documents_from_docket_ids(docket_ids, days=lookback_days)
    merged_docs = {}
    for d in (cl_docs + extra_docs):
        key = (d.docket_id, d.doc_number, d.date_filed, d.document_url)
        merged_docs[key] = d
    cl_docs = list(merged_docs.values())

    docket_case_count = len(cl_cases)
    
    # =====================================================
    # FIX: RECAP ë¬¸ì„œ ê±´ìˆ˜ ê³„ì‚° ë°©ì‹ ìˆ˜ì •
    # í•´ê²°: cl_docsì— ìˆëŠ” ê²ƒ + cl_cases ì¤‘ complaint_linkê°€ ìˆëŠ” Docket ID í•©ì‚°
    # =====================================================
    unique_dockets_with_docs = set()
    for d in cl_docs:
        if d.docket_id:
            unique_dockets_with_docs.add(d.docket_id)
    for c in cl_cases:
        if c.complaint_link and c.docket_id:
            unique_dockets_with_docs.add(c.docket_id)
    
    recap_doc_count = len(unique_dockets_with_docs)

    # 3) ë Œë”ë§
    md = render_markdown(
        lawsuits,
        cl_docs,
        cl_cases,
        recap_doc_count,
        lookback_days=lookback_days,
    )    
    # 4) GitHub Issue ì‘ì—…
    issue_no = find_or_create_issue(owner, repo, gh_token, issue_title, issue_label)
    issue_url = f"https://github.com/{owner}/{repo}/issues/{issue_no}"
   

    # =========================================================
    # Baseline ë¹„êµ ë¡œì§ (Modularized)
    # =========================================================
    comments = list_comments(owner, repo, gh_token, issue_no)
    md = apply_deduplication(md, comments)
    
    # ì‹¤í–‰ ì‹œê°(KST)ì„ ìµœìƒë‹¨ì— ë°°ì¹˜ (ì¤‘ë³µ ì œê±° ìš”ì•½ë³´ë‹¤ ìœ„ì— ì˜¤ë„ë¡)
    md = f"### ì‹¤í–‰ ì‹œê°(KST): {run_ts_kst}\n\n" + md

    # ì´ì „ ë‚ ì§œ ì´ìŠˆ Close
    closed_nums = close_other_daily_issues(owner, repo, gh_token, issue_label, base_title, issue_title, issue_no, issue_url)
    if closed_nums:
        debug_log(f"ì´ì „ ë‚ ì§œ ì´ìŠˆ ìë™ Close: {closed_nums}")
    
    debug_log(f"ğŸ“Š ìˆ˜ì§‘ ë° ë¶„ì„ ì™„ë£Œ (ìµœê·¼ {lookback_days}ì¼)")
    debug_log(f"  â”œ News: {len(lawsuits)}ê±´")
    debug_log(f"  â”” Cases (CourtListener+RECAP): {docket_case_count}ê±´ (ë¬¸ì„œ {recap_doc_count}ê±´)")

    debug_log("===== REPORT PREVIEW (First 1000 chars) =====")
    debug_log(md[:1000])
    debug_log(f"Report full length: {len(md)}")

    # KST ê¸°ì¤€ íƒ€ì„ìŠ¤íƒ¬í”„
    timestamp = datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M KST")

    comment_body = f"\n\n{md}"
    create_comment(owner, repo, gh_token, issue_no, comment_body)
    debug_log(f"Issue #{issue_no} ëŒ“ê¸€ ì—…ë¡œë“œ ì™„ë£Œ")

    # 5) Slack ìš”ì•½ ì „ì†¡
    # ============================================
    # Slack ì¶œë ¥ ê°œì„  (ìµœì¢… í¬ë§·)
    # ============================================

    slack_dedup_news = None
    slack_dedup_cases = None

    if "### ì¤‘ë³µ ì œê±° ìš”ì•½:" in md:
        m_news = re.search(
            r"â”” News (.+)",
            md,
        )

        m_cases = re.search(
            r"â”” Cases (.+)",
            md,
        )

        if m_news:
            line = m_news.group(1).strip()
            # GitHubìš© ê°•ì¡°(**)ì™€ ğŸ”´ ì œê±° (Slackìš©ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ê¸° ìœ„í•¨)
            line = line.replace("**", "").replace(" ğŸ”´", "")
            # New ìˆ˜ì¹˜ê°€ 0ë³´ë‹¤ í¬ë©´ ê°•ì¡° (Bolding + ğŸ”´)
            slack_dedup_news = re.sub(
                r"(\d+)\s+\(New\)",
                lambda m: f"*{m.group(1)} (New)*" + (" ğŸ”´" if int(m.group(1)) > 0 else ""),
                line
            )

        if m_cases:
            line = m_cases.group(1).strip()
            # GitHubìš© ê°•ì¡°(**)ì™€ ğŸ”´ ì œê±°
            line = line.replace("**", "").replace(" ğŸ”´", "")
            slack_dedup_cases = re.sub(
                r"(\d+)\s+\(New\)",
                lambda m: f"*{m.group(1)} (New)*" + (" ğŸ”´" if int(m.group(1)) > 0 else ""),
                line
            )



    slack_lines = []

    slack_lines.append("ğŸ“Š AI ì†Œì†¡ ëª¨ë‹ˆí„°ë§")
    slack_lines.append(f"ğŸ•’ {timestamp}")
    slack_lines.append("")

    # ğŸ” Dedup Summary
    if slack_dedup_news and slack_dedup_cases:
        slack_lines.append("ğŸ” Dedup Summary")
        slack_lines.append(f"â”” News {slack_dedup_news}")
        slack_lines.append(f"â”” Cases {slack_dedup_cases}")
        slack_lines.append("")

    # ğŸ“ˆ Collection Status
    slack_lines.append("ğŸ“ˆ Collection Status")
    slack_lines.append(f"â”” News: {len(lawsuits)}")
    slack_lines.append(
        f"â”” Cases: {docket_case_count} (Docs: {recap_doc_count})"
    )
    slack_lines.append("")

    # ğŸ”— GitHub
    slack_lines.append(f"ğŸ”— GitHub: <{issue_url}|#{issue_no}>")

    # ğŸ†• ìµœì‹  RECAP ë¬¸ì„œ
    if cl_docs:
        top = sorted(
            cl_docs,
            key=lambda x: getattr(x, "date_filed", ""),
            reverse=True,
        )[:3]

        slack_lines.append("")
        slack_lines.append("ğŸ†• ìµœì‹  RECAP ë¬¸ì„œ")

        for d in top:
            date = getattr(d, "date_filed", "N/A")
            name = getattr(d, "case_name", "Unknown Case")
            docket_id = getattr(d, "docket_id", None) 
            absolute_url = getattr(d, "absolute_url", None)

            if absolute_url:
                # ê°€ì¥ ì •í™•í•œ URL (slug í¬í•¨)
                docket_url = absolute_url
                if not docket_url.endswith("/"):
                    docket_url += "/"

                slack_lines.append(
                    f"â€¢ {date} | <{docket_url}|{name}>"
                )
            elif docket_id:
                # slug ìƒì„± (utilsì˜ ê³µí†µ í•¨ìˆ˜ ì‚¬ìš©)
                slug = slugify_case_name(name)

                docket_url = (
                    f"https://www.courtlistener.com/docket/"
                    f"{docket_id}/{slug}/"
                )

                slack_lines.append(
                    f"â€¢ {date} | <{docket_url}|{name}>"
                )
            else:
                slack_lines.append(f"â€¢ {date} | {name}")
    try:
        post_to_slack(slack_webhook, "\n".join(slack_lines))
        debug_log(f"Slack ì „ì†¡ ì™„ë£Œ")
    except Exception as e:
        debug_log(f"Slack ì „ì†¡ ì‹¤íŒ¨: {e}")
        
if __name__ == "__main__":
    main()
