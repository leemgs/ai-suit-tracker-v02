from __future__ import annotations
import os
from datetime import datetime, timezone
from zoneinfo import ZoneInfo

from .fetch import fetch_news
from .extract import load_known_cases, build_lawsuits_from_news
from .render import render_markdown
from .github_issue import find_or_create_issue, create_comment, close_other_daily_issues
from .github_issue import list_comments, get_first_comment_body
from .slack import post_to_slack
from .courtlistener import (
    search_recent_documents,
    build_complaint_documents_from_hits,
    build_case_summaries_from_hits,
    build_case_summaries_from_docket_numbers,
    build_case_summaries_from_case_titles,
    build_documents_from_docket_ids,
)
from .queries import COURTLISTENER_QUERIES

def main() -> None:
    # 0) í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    owner = os.environ["GITHUB_OWNER"]
    repo = os.environ["GITHUB_REPO"]
    gh_token = os.environ["GITHUB_TOKEN"]
    slack_webhook = os.environ["SLACK_WEBHOOK_URL"]

    base_title = os.environ.get("ISSUE_TITLE_BASE", "AI ë¶ˆë²•/ë¬´ë‹¨ í•™ìŠµë°ì´í„° ì†Œì†¡ ëª¨ë‹ˆí„°ë§")
    lookback_days = int(os.environ.get("LOOKBACK_DAYS", "3"))
    # í•„ìš” ì‹œ 2ë¡œ ë³€ê²½: í™˜ê²½ë³€ìˆ˜ LOOKBACK_DAYS=2
    
    # KST ê¸°ì¤€ ë‚ ì§œ ìƒì„±
    now_kst = datetime.now(ZoneInfo("Asia/Seoul"))
    run_ts_kst = now_kst.strftime("%Y-%m-%d %H:%M")
    issue_day_kst = now_kst.strftime("%Y-%m-%d")
    issue_title = f"{base_title} ({issue_day_kst})"
    print(f"KST ê¸°ì¤€ ì‹¤í–‰ì‹œê°: {run_ts_kst}")
    
    issue_label = os.environ.get("ISSUE_LABEL", "ai-lawsuit-monitor")

    # 1) CourtListener ê²€ìƒ‰
    hits = []
    for q in COURTLISTENER_QUERIES:
        hits.extend(search_recent_documents(q, days=lookback_days, max_results=20))
    
    # ì¤‘ë³µ ì œê±°
    dedup = {}
    for h in hits:
        key = (h.get("absolute_url") or h.get("url") or "") + "|" + (h.get("caseName") or h.get("title") or "")
        dedup[key] = h
    hits = list(dedup.values())

    cl_docs = build_complaint_documents_from_hits(hits, days=lookback_days)
    # RECAP ë„ì¼“(ì‚¬ê±´) ìš”ì•½: "ë²•ì› ì‚¬ê±´(ë„ì¼“) í™•ì¸ ê±´ìˆ˜"ë¡œ ì‚¬ìš©
    cl_cases = build_case_summaries_from_hits(hits)

    # 2) ë‰´ìŠ¤ ìˆ˜ì§‘
    news = fetch_news()
    known = load_known_cases()
    lawsuits = build_lawsuits_from_news(news, known, lookback_days=lookback_days)

    # 2-1) ë‰´ìŠ¤ í…Œì´ë¸”ì˜ ì†Œì†¡ë²ˆí˜¸(ë„ì¼“ë²ˆí˜¸)ë¡œ RECAP ë„ì¼“/ë¬¸ì„œ í™•ìž¥
    docket_numbers = [s.case_number for s in lawsuits if (s.case_number or "").strip() and s.case_number != "ë¯¸í™•ì¸"]
    extra_cases = build_case_summaries_from_docket_numbers(docket_numbers)

    # 2-2) ì†Œì†¡ë²ˆí˜¸ê°€ ì—†ë”ë¼ë„, 'ì†Œì†¡ì œëª©'(ì¶”ì • ì¼€ì´ìŠ¤ëª…)ìœ¼ë¡œ ë„ì¼“ í™•ìž¥
    case_titles = [s.case_title for s in lawsuits if (s.case_title or "").strip() and s.case_title != "ë¯¸í™•ì¸"]
    extra_cases_by_title = build_case_summaries_from_case_titles(case_titles)

    merged_cases = {c.docket_id: c for c in (cl_cases + extra_cases + extra_cases_by_title)}
    cl_cases = list(merged_cases.values())

    # ë¬¸ì„œë„ docket id ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ ì‹œë„(Complaint ìš°ì„ , ì—†ìœ¼ë©´ fallback)
    docket_ids = list(merged_cases.keys())
    extra_docs = build_documents_from_docket_ids(docket_ids, days=lookback_days)
    merged_docs = {}
    for d in (cl_docs + extra_docs):
        key = (d.docket_id, d.doc_number, d.date_filed, d.document_url)
        merged_docs[key] = d
    cl_docs = list(merged_docs.values())

    docket_case_count = len(cl_cases)
    
    # =====================================================
    # ðŸ”¥ FIX: RECAP ë¬¸ì„œ ê±´ìˆ˜ ê³„ì‚° ë°©ì‹ ìˆ˜ì •
    # ê¸°ì¡´: len(cl_docs)
    # ë¬¸ì œ: HTML fallback ë“±ìœ¼ë¡œ CLCaseSummaryì—ë§Œ complaint_linkê°€ ìžˆê³ 
    #       CLDocumentê°€ ìƒì„±ë˜ì§€ ì•ŠëŠ” ê²½ìš° KPIê°€ 0ìœ¼ë¡œ ë‚˜ì˜´
    # í•´ê²°: CLCaseSummary ê¸°ì¤€ìœ¼ë¡œ complaint_link ì¡´ìž¬ ì—¬ë¶€ ì¹´ìš´íŠ¸
    # =====================================================
    recap_doc_count = sum(
        1 for c in cl_cases
        if (getattr(c, "complaint_link", "") or "").strip()
    )

    # 3) ë Œë”ë§
    md = render_markdown(
        lawsuits,
        cl_docs,
        cl_cases,
        recap_doc_count,
        lookback_days=lookback_days,
    )    
    md = f"### ì‹¤í–‰ ì‹œê°(KST): {run_ts_kst}\n\n" + md
    
    print("===== REPORT BEGIN =====")
    print(md[:1000]) # ë¡œê·¸ ë„ˆë¬´ ê¸¸ë©´ ìž˜ë¦¬ë¯€ë¡œ ì¼ë¶€ë§Œ ì¶œë ¥
    print("===== REPORT END =====")

    # 4) GitHub Issue ìž‘ì—…
    issue_no = find_or_create_issue(owner, repo, gh_token, issue_title, issue_label)
    issue_url = f"https://github.com/{owner}/{repo}/issues/{issue_no}"
   

    # =========================================================
    # ðŸ”¥ Base Snapshot ë¹„êµ ë¡œì§
    # =========================================================
    comments = list_comments(owner, repo, gh_token, issue_no)
    first_run_today = len(comments) == 0

    if not first_run_today:
        base_body = get_first_comment_body(owner, repo, gh_token, issue_no) or ""

        import re

        # =====================================================
        # ðŸ”’ ì•ˆì •í˜• í…Œì´ë¸” ê¸°ë°˜ ë¹„êµ ë¡œì§
        # =====================================================

        def extract_section(md_text: str, section_title: str) -> str:
            lines = md_text.split("\n")
            start = None
            end = None
            for i, line in enumerate(lines):
                if line.strip().startswith(section_title):
                    start = i + 1
                    continue
                if start and line.startswith("## "):
                    end = i
                    break
            if start is None:
                return ""
            if end is None:
                end = len(lines)
            return "\n".join(lines[start:end])

        def parse_table(section_md: str):
            lines = [l for l in section_md.split("\n") if l.strip().startswith("|")]
            if len(lines) < 3:
                return [], [], ()

            header = lines[0]
            separator = lines[1]
            rows = lines[2:]

            header_cols = [c.strip() for c in header.split("|")[1:-1]]

            parsed_rows = []
            for row in rows:
                cols = [c.strip() for c in row.split("|")[1:-1]]
                if len(cols) == len(header_cols):
                    parsed_rows.append(cols)

            return header_cols, parsed_rows, (header, separator)

        def extract_article_url(cell: str):
            m = re.search(r"\((https?://[^\)]+)\)", cell)
            if m:
                return m.group(1).split("&hl=")[0]
            return None

        # -------------------------
        # Base Snapshot Key Set ìƒì„±
        # -------------------------
        base_article_set = set()
        base_docket_set = set()

        news_section_base = extract_section(base_body, "## ðŸ“° ì™¸ë¶€ ê¸°ì‚¬ ê¸°ë°˜ ì†Œì†¡ ì •ë³´")
        headers, rows, _ = parse_table(news_section_base)
        if "ì œëª©" in headers:
            idx = headers.index("ì œëª©")
            for r in rows:
                url = extract_article_url(r[idx])
                if url:
                    base_article_set.add(url)

        recap_section_base = extract_section(base_body, "## âš–ï¸ RECAP")
        headers, rows, _ = parse_table(recap_section_base)
        if "ë„ì¼“ë²ˆí˜¸" in headers:
            idx = headers.index("ë„ì¼“ë²ˆí˜¸")
            for r in rows:
                base_docket_set.add(r[idx])

        # -------------------------
        # í˜„ìž¬ md ì²˜ë¦¬
        # -------------------------
        current_md = md

        # ì™¸ë¶€ ê¸°ì‚¬ ì²˜ë¦¬
        news_section = extract_section(current_md, "## ðŸ“° ì™¸ë¶€ ê¸°ì‚¬ ê¸°ë°˜ ì†Œì†¡ ì •ë³´")
        headers, rows, table_meta = parse_table(news_section)

        new_article_count = 0
        total_article_count = len(rows)

        if headers and "ì œëª©" in headers:
            idx = headers.index("ì œëª©")
            header_line, separator_line = table_meta
            new_lines = [header_line, separator_line]

            for r in rows:
                url = extract_article_url(r[idx])
                if url in base_article_set:
                    # ðŸ”¥ ê°œì„ : í•µì‹¬ ì‹ë³„ ì»¬ëŸ¼(No, ê¸°ì‚¬ì¼ìž, ì œëª©)ì€ ìœ ì§€
                    try:
                        no_idx = headers.index("No.")
                        date_idx = headers.index("ê¸°ì‚¬ì¼ìžâ¬‡ï¸")
                        title_idx = headers.index("ì œëª©")
                    except ValueError:
                        no_idx = date_idx = title_idx = None

                    new_row = []
                    for i, col in enumerate(r):
                        if i in (no_idx, date_idx, title_idx):
                            new_row.append(col)
                        else:
                            new_row.append("skip")

                    new_lines.append("| " + " | ".join(new_row) + " |")
                else:
                    new_lines.append("| " + " | ".join(r) + " |")
                    new_article_count += 1

            new_news_section = "\n".join(new_lines)
            current_md = current_md.replace(news_section, new_news_section)

        # RECAP ì²˜ë¦¬
        recap_section = extract_section(current_md, "## âš–ï¸ RECAP")
        headers, rows, table_meta = parse_table(recap_section)

        new_docket_count = 0
        total_docket_count = len(rows)

        if headers and "ë„ì¼“ë²ˆí˜¸" in headers:
            idx = headers.index("ë„ì¼“ë²ˆí˜¸")
            header_line, separator_line = table_meta
            new_lines = [header_line, separator_line]

            for r in rows:
                docket = r[idx]
                if docket in base_docket_set:
                    # ðŸ”¥ ê°œì„ : í•µì‹¬ ì‹ë³„ ì»¬ëŸ¼(No, ìƒíƒœ, ì¼€ì´ìŠ¤ëª…, ë„ì¼“ë²ˆí˜¸) ìœ ì§€
                    try:
                        no_idx = headers.index("No.")
                        status_idx = headers.index("ìƒíƒœ")
                        case_idx = headers.index("ì¼€ì´ìŠ¤ëª…")
                        docket_idx = headers.index("ë„ì¼“ë²ˆí˜¸")
                    except ValueError:
                        no_idx = status_idx = case_idx = docket_idx = None

                    new_row = []
                    for i, col in enumerate(r):
                        if i in (no_idx, status_idx, case_idx, docket_idx):
                            new_row.append(col)
                        else:
                            new_row.append("skip")

                    new_lines.append("| " + " | ".join(new_row) + " |")
                else:
                    new_lines.append("| " + " | ".join(r) + " |")
                    new_docket_count += 1

            new_recap_section = "\n".join(new_lines)
            current_md = current_md.replace(recap_section, new_recap_section)

        # -------------------------
        # Summary ìƒì„±
        # -------------------------
        summary_header = (
            "### ìžë£Œ ì¤‘ë³µ ì œê±° ê²°ê³¼ ìš”ì•½:\n"
            f"1). ì™¸ë¶€ ê¸°ì‚¬ ê¸°ë°˜ ì†Œì†¡ ì •ë³´: ê¸°ì¡´ {len(base_article_set)}ê±´ (base snapshot) "
            f"+ ì‹ ê·œ {new_article_count}ê±´ = ì´ {total_article_count}ê±´\n"
            f"2). RECAP: ê¸°ì¡´ {len(base_docket_set)}ê±´ (base snapshot) "
            f"+ ì‹ ê·œ {new_docket_count}ê±´ = ì´ {total_docket_count}ê±´\n\n"
        )

        md = summary_header + current_md 

    # ì´ì „ ë‚ ì§œ ì´ìŠˆ Close
    closed_nums = close_other_daily_issues(owner, repo, gh_token, issue_label, base_title, issue_title, issue_no, issue_url)
    if closed_nums:
        print(f"ì´ì „ ë‚ ì§œ ì´ìŠˆ ìžë™ Close: {closed_nums}")
    
    # KST ê¸°ì¤€ íƒ€ìž„ìŠ¤íƒ¬í”„
    timestamp = datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M KST")

    comment_body = f"\n\n{md}"
    create_comment(owner, repo, gh_token, issue_no, comment_body)
    print(f"Issue #{issue_no} ëŒ“ê¸€ ì—…ë¡œë“œ ì™„ë£Œ")

    # 5) Slack ìš”ì•½ ì „ì†¡
    # ============================================
    # ðŸ”¥ Slack ì¶œë ¥ ê°œì„  (ìµœì¢… í¬ë§·)
    # ============================================

    import re

    base_news = new_news = total_news = None
    base_cases = new_cases = total_cases = None

    if "### ìžë£Œ ì¤‘ë³µ ì œê±° ê²°ê³¼ ìš”ì•½:" in md:

        m_news = re.search(
            r"ì™¸ë¶€ ê¸°ì‚¬ ê¸°ë°˜ ì†Œì†¡ ì •ë³´: ê¸°ì¡´ (\d+)ê±´ .*?\+ ì‹ ê·œ (-?\d+)ê±´ = ì´ (\d+)ê±´",
            md,
        )

        m_cases = re.search(
            r"RECAP: ê¸°ì¡´ (\d+)ê±´ .*?\+ ì‹ ê·œ (-?\d+)ê±´ = ì´ (\d+)ê±´",
            md,
        )

        if m_news:
            base_news = int(m_news.group(1))
            new_news = int(m_news.group(2))
            total_news = int(m_news.group(3))

        if m_cases:
            base_cases = int(m_cases.group(1))
            new_cases = int(m_cases.group(2))
            total_cases = int(m_cases.group(3))

    def format_delta(n: int) -> str:
        if n > 0:
            return f"+{n}"
        elif n < 0:
            return f"{n}"
        else:
            return "0"

    slack_lines = []

    slack_lines.append("ðŸ“Š AI ì†Œì†¡ ëª¨ë‹ˆí„°ë§")
    slack_lines.append(f"ðŸ•’ {timestamp}")
    slack_lines.append("")

    # ðŸ” Dedup Summary
    if base_news is not None and base_cases is not None:
        slack_lines.append("ðŸ” Dedup Summary")
        slack_lines.append(
            f"â”” News: {base_news} â†’ {format_delta(new_news)} = {total_news}"
        )
        slack_lines.append(
            f"â”” Cases: {base_cases} â†’ {format_delta(new_cases)} = {total_cases}"
        )
        slack_lines.append("")

    # ðŸ“ˆ Collection Status
    slack_lines.append("ðŸ“ˆ Collection Status")
    slack_lines.append(f"â”” News: {len(lawsuits)}")
    slack_lines.append(
        f"â”” Cases: {docket_case_count} (Docs: {recap_doc_count})"
    )
    slack_lines.append("")

    # ðŸ”— GitHub
    slack_lines.append(f"ðŸ”— GitHub: <{issue_url}|#{issue_no}>")

    # ðŸ†• ìµœì‹  RECAP ë¬¸ì„œ
    if cl_docs:
        top = sorted(
            cl_docs,
            key=lambda x: getattr(x, "date_filed", ""),
            reverse=True,
        )[:3]

        slack_lines.append("")
        slack_lines.append("ðŸ†• ìµœì‹  RECAP ë¬¸ì„œ")

        for d in top:
            date = getattr(d, "date_filed", "N/A")
            name = getattr(d, "case_name", "Unknown Case")
            docket_id = getattr(d, "docket_id", None)

            if docket_id:
                docket_url = f"https://www.courtlistener.com/docket/{docket_id}/"
                slack_lines.append(
                    f"â€¢ {date} | <{docket_url}|{name}>"
                )
            else:
                slack_lines.append(f"â€¢ {date} | {name}")

    post_to_slack(slack_webhook, "\n".join(slack_lines))
    print("Slack ì „ì†¡ ì™„ë£Œ")

if __name__ == "__main__":
    main()
