from __future__ import annotations
import os
import re
from datetime import datetime, timezone
from zoneinfo import ZoneInfo

from .fetch import fetch_news
from .extract import load_known_cases, build_lawsuits_from_news
from .render import render_markdown
from .github_issue import find_or_create_issue, create_comment, close_other_daily_issues
from .github_issue import list_comments
from .slack import post_to_slack
from .utils import debug_log
from .courtlistener import (
    search_recent_documents,
    build_complaint_documents_from_hits,
    build_case_summaries_from_hits,
    build_case_summaries_from_docket_numbers,
    build_case_summaries_from_case_titles,
    build_documents_from_docket_ids,
)
from .queries import COURTLISTENER_QUERIES

def main() -> None:
    # 0) í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    owner = os.environ.get("GITHUB_OWNER")
    repo = os.environ.get("GITHUB_REPO")
    gh_token = os.environ.get("GITHUB_TOKEN")
    slack_webhook = os.environ.get("SLACK_WEBHOOK_URL")

    if not all([owner, repo, gh_token, slack_webhook]):
        missing = [k for k, v in {"GITHUB_OWNER": owner, "GITHUB_REPO": repo, "GITHUB_TOKEN": gh_token, "SLACK_WEBHOOK_URL": slack_webhook}.items() if not v]
        raise ValueError(f"í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing)}")

    base_title = os.environ.get("ISSUE_TITLE_BASE", "AI ë¶ˆë²•/ë¬´ë‹¨ í•™ìŠµë°ì´í„° ì†Œì†¡ ëª¨ë‹ˆí„°ë§")
    lookback_days = int(os.environ.get("LOOKBACK_DAYS", "3"))
    # í•„ìš” ì‹œ 2ë¡œ ë³€ê²½: í™˜ê²½ë³€ìˆ˜ LOOKBACK_DAYS=2
    
    # KST ê¸°ì¤€ ë‚ ì§œ ìƒì„±
    now_kst = datetime.now(ZoneInfo("Asia/Seoul"))
    run_ts_kst = now_kst.strftime("%Y-%m-%d %H:%M")
    issue_day_kst = now_kst.strftime("%Y-%m-%d")
    issue_title = f"{base_title} ({issue_day_kst})"
    debug_log(f"KST ê¸°ì¤€ ì‹¤í–‰ì‹œê°: {run_ts_kst}")
    
    issue_label = os.environ.get("ISSUE_LABEL", "ai-lawsuit-monitor")

    # 1) CourtListener ê²€ìƒ‰
    hits = []
    for q in COURTLISTENER_QUERIES:
        debug_log(f"Running CourtListener query: {q}")
        hits.extend(search_recent_documents(q, days=lookback_days, max_results=20))
    
    # ì¤‘ë³µ ì œê±°
    dedup = {}
    for h in hits:
        key = (h.get("absolute_url") or h.get("url") or "") + "|" + (h.get("caseName") or h.get("title") or "")
        dedup[key] = h
    hits = list(dedup.values())

    cl_docs = build_complaint_documents_from_hits(hits, days=lookback_days)
    # RECAP ë„ì¼“(ì‚¬ê±´) ìš”ì•½: "ë²•ì› ì‚¬ê±´(ë„ì¼“) í™•ì¸ ê±´ìˆ˜"ë¡œ ì‚¬ìš©
    cl_cases = build_case_summaries_from_hits(hits)

    # 2) ë‰´ìŠ¤ ìˆ˜ì§‘
    news = fetch_news()
    known = load_known_cases()
    lawsuits = build_lawsuits_from_news(news, known, lookback_days=lookback_days)

    # 2-1) ë‰´ìŠ¤ í…Œì´ë¸”ì˜ ì†Œì†¡ë²ˆí˜¸(ë„ì¼“ë²ˆí˜¸)ë¡œ RECAP ë„ì¼“/ë¬¸ì„œ í™•ì¥
    docket_numbers = [s.case_number for s in lawsuits if (s.case_number or "").strip() and s.case_number != "ë¯¸í™•ì¸"]
    extra_cases = build_case_summaries_from_docket_numbers(docket_numbers)

    # 2-2) ì†Œì†¡ë²ˆí˜¸ê°€ ì—†ë”ë¼ë„, 'ì†Œì†¡ì œëª©'(ì¶”ì • ì¼€ì´ìŠ¤ëª…)ìœ¼ë¡œ ë„ì¼“ í™•ì¥
    case_titles = [s.case_title for s in lawsuits if (s.case_title or "").strip() and s.case_title != "ë¯¸í™•ì¸"]
    extra_cases_by_title = build_case_summaries_from_case_titles(case_titles)

    merged_cases = {c.docket_id: c for c in (cl_cases + extra_cases + extra_cases_by_title)}
    cl_cases = list(merged_cases.values())

    # ë¬¸ì„œë„ docket id ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ ì‹œë„(Complaint ìš°ì„ , ì—†ìœ¼ë©´ fallback)
    docket_ids = list(merged_cases.keys())
    extra_docs = build_documents_from_docket_ids(docket_ids, days=lookback_days)
    merged_docs = {}
    for d in (cl_docs + extra_docs):
        key = (d.docket_id, d.doc_number, d.date_filed, d.document_url)
        merged_docs[key] = d
    cl_docs = list(merged_docs.values())

    docket_case_count = len(cl_cases)
    
    # =====================================================
    # FIX: RECAP ë¬¸ì„œ ê±´ìˆ˜ ê³„ì‚° ë°©ì‹ ìˆ˜ì •
    # ê¸°ì¡´: len(cl_docs)
    # ë¬¸ì œ: HTML fallback ë“±ìœ¼ë¡œ CLCaseSummaryì—ë§Œ complaint_linkê°€ ìˆê³ 
    #       CLDocumentê°€ ìƒì„±ë˜ì§€ ì•ŠëŠ” ê²½ìš° KPIê°€ 0ìœ¼ë¡œ ë‚˜ì˜´
    # í•´ê²°: CLCaseSummary ê¸°ì¤€ìœ¼ë¡œ complaint_link ì¡´ì¬ ì—¬ë¶€ ì¹´ìš´íŠ¸
    # =====================================================
    recap_doc_count = len(cl_docs)

    # 3) ë Œë”ë§
    md = render_markdown(
        lawsuits,
        cl_docs,
        cl_cases,
        recap_doc_count,
        lookback_days=lookback_days,
    )    
    md = f"### ì‹¤í–‰ ì‹œê°(KST): {run_ts_kst}\n\n" + md
    
    debug_log(f"ğŸ“Š ìˆ˜ì§‘ ë° ë¶„ì„ ì™„ë£Œ (ìµœê·¼ {lookback_days}ì¼)")
    debug_log(f"  â”œ News: {len(lawsuits)}ê±´")
    debug_log(f"  â”” Cases (CourtListener+RECAP): {docket_case_count}ê±´ (ë¬¸ì„œ {recap_doc_count}ê±´)")

    debug_log("===== REPORT PREVIEW (First 1000 chars) =====")
    debug_log(md[:1000])
    debug_log(f"Report full length: {len(md)}")

    # 4) GitHub Issue ì‘ì—…
    issue_no = find_or_create_issue(owner, repo, gh_token, issue_title, issue_label)
    issue_url = f"https://github.com/{owner}/{repo}/issues/{issue_no}"
   

    # =========================================================
    # Baseline ë¹„êµ ë¡œì§
    # =========================================================
    comments = list_comments(owner, repo, gh_token, issue_no)
    first_run_today = len(comments) == 0

    if not first_run_today:
        # =====================================================
        # ğŸ”’ ì•ˆì •í˜• í…Œì´ë¸” ê¸°ë°˜ ë¹„êµ ë¡œì§ (ëª¨ë“  ì´ì „ ëŒ“ê¸€ ëŒ€ìƒ)
        # =====================================================

        def extract_section(md_text: str, section_title: str) -> str:
            lines = md_text.split("\n")
            start = None
            end = None
            for i, line in enumerate(lines):
                if line.strip().startswith(section_title):
                    start = i + 1
                    continue
                if start and line.startswith("## "):
                    end = i
                    break
            if start is None:
                return ""
            if end is None:
                end = len(lines)
            return "\n".join(lines[start:end])

        def parse_table(section_md: str):
            lines = [l for l in section_md.split("\n") if l.strip().startswith("|")]
            if len(lines) < 3:
                return [], [], ()

            header = lines[0]
            separator = lines[1]
            rows = lines[2:]

            def split_row(row_text: str):
                # ì •ê·œì‹ (?<!\\)\| ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—­ìŠ¬ë˜ì‹œë¡œ ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ íŒŒì´í”„ë§Œ ë¶„í• 
                return [c.strip() for c in re.split(r'(?<!\\)\|', row_text.strip())[1:-1]]

            header_cols = split_row(header)

            parsed_rows = []
            for row in rows:
                cols = split_row(row)
                if len(cols) == len(header_cols):
                    parsed_rows.append(cols)
                else:
                    debug_log(f"Table row column mismatch: expected {len(header_cols)}, got {len(cols)}. Row: {row[:100]}...")

            return header_cols, parsed_rows, (header, separator)

        def extract_article_url(cell: str):
            m = re.search(r"\((https?://[^\)]+)\)", cell)
            if m:
                return m.group(1).split("&hl=")[0]
            return None

        # -------------------------
        # Base Snapshot Key Set ìƒì„± (ëª¨ë“  ì´ì „ ëŒ“ê¸€ ëŒ€ìƒ)
        # -------------------------
        base_article_set = set()
        base_docket_set = set()

        for comment in comments:
            body = comment.get("body") or ""
            
            # News ì²˜ë¦¬
            news_section_base = extract_section(body, "## ğŸ“° News")
            h_news, r_news, _ = parse_table(news_section_base)
            if "ì œëª©" in h_news:
                idx = h_news.index("ì œëª©")
                for r in r_news:
                    url = extract_article_url(r[idx])
                    if url:
                        base_article_set.add(url)
            
            # Cases ì²˜ë¦¬
            recap_section_base = extract_section(body, "## âš–ï¸ Cases")
            h_cases, r_cases, _ = parse_table(recap_section_base)
            if "ë„ì¼“ë²ˆí˜¸" in h_cases:
                idx = h_cases.index("ë„ì¼“ë²ˆí˜¸")
                for r in r_cases:
                    base_docket_set.add(r[idx])

        # -------------------------
        # í˜„ì¬ md ì²˜ë¦¬
        # -------------------------
        current_md = md

        # ì™¸ë¶€ ê¸°ì‚¬ ì²˜ë¦¬
        news_section = extract_section(current_md, "## ğŸ“° News")
        headers, rows, table_meta = parse_table(news_section)

        new_article_count = 0
        total_article_count = len(rows)

        if headers and "ì œëª©" in headers:
            title_idx = headers.index("ì œëª©")
            no_idx = headers.index("No.") if "No." in headers else None
            date_idx = headers.index("ê¸°ì‚¬ì¼ìâ¬‡ï¸") if "ê¸°ì‚¬ì¼ìâ¬‡ï¸" in headers else None

            header_line, separator_line = table_meta
            
            non_skip_rows = []
            skip_rows = []

            for r in rows:
                url = extract_article_url(r[title_idx])
                if url in base_article_set:
                    # ê°œì„ : í•µì‹¬ ì‹ë³„ ì»¬ëŸ¼(No, ê¸°ì‚¬ì¼ì, ì œëª©)ì€ ìœ ì§€
                    new_row = []
                    for i, col in enumerate(r):
                        if i in (no_idx, date_idx, title_idx):
                            new_row.append(col)
                        else:
                            new_row.append("skip")
                    skip_rows.append(new_row)
                else:
                    non_skip_rows.append(r)
                    new_article_count += 1
            
            # í•©ì¹˜ê¸°: ì‹ ê·œ ë¨¼ì €, ê¸°ì¡´(skip) ë‚˜ì¤‘
            final_rows = non_skip_rows + skip_rows
            new_lines = [header_line, separator_line]
            
            for row_idx, r in enumerate(final_rows, start=1):
                if no_idx is not None:
                    r[no_idx] = str(row_idx)
                new_lines.append("| " + " | ".join(r) + " |")

            new_news_section = "\n".join(new_lines)
            current_md = current_md.replace(news_section, new_news_section)

        # Cases ì²˜ë¦¬
        recap_section = extract_section(current_md, "## âš–ï¸ Cases")
        headers, rows, table_meta = parse_table(recap_section)

        new_docket_count = 0
        total_docket_count = len(rows)

        if headers and "ë„ì¼“ë²ˆí˜¸" in headers:
            docket_idx = headers.index("ë„ì¼“ë²ˆí˜¸")
            no_idx = headers.index("No.") if "No." in headers else None
            status_idx = headers.index("ìƒíƒœ") if "ìƒíƒœ" in headers else None
            case_idx = headers.index("ì¼€ì´ìŠ¤ëª…") if "ì¼€ì´ìŠ¤ëª…" in headers else None

            header_line, separator_line = table_meta
            
            non_skip_rows = []
            skip_rows = []

            for r in rows:
                docket = r[docket_idx]
                if docket in base_docket_set:
                    # ê°œì„ : í•µì‹¬ ì‹ë³„ ì»¬ëŸ¼(No, ìƒíƒœ, ì¼€ì´ìŠ¤ëª…, ë„ì¼“ë²ˆí˜¸) ìœ ì§€
                    new_row = []
                    for i, col in enumerate(r):
                        if i in (no_idx, status_idx, case_idx, docket_idx):
                            new_row.append(col)
                        else:
                            new_row.append("skip")
                    skip_rows.append(new_row)
                else:
                    non_skip_rows.append(r)
                    new_docket_count += 1

            # í•©ì¹˜ê¸°: ì‹ ê·œ ë¨¼ì €, ê¸°ì¡´(skip) ë‚˜ì¤‘
            final_rows = non_skip_rows + skip_rows
            new_lines = [header_line, separator_line]
            
            for row_idx, r in enumerate(final_rows, start=1):
                if no_idx is not None:
                    r[no_idx] = str(row_idx)
                new_lines.append("| " + " | ".join(r) + " |")

            new_recap_section = "\n".join(new_lines)
            current_md = current_md.replace(recap_section, new_recap_section)

        # -------------------------
        # Summary ìƒì„±
        # -------------------------
        base_news = len(base_article_set)
        base_cases = len(base_docket_set)

        dup_news = total_article_count - new_article_count
        dup_cases = total_docket_count - new_docket_count

        summary_header = (
            "### ì¤‘ë³µ ì œê±° ìš”ì•½:\n"
            "ğŸ” Dedup Summary\n"
            f"â”” News {base_news} (Baseline): "
            f"{dup_news} (Dup), "
            f"{new_article_count} (New)\n"
            f"â”” Cases {base_cases} (Baseline): "
            f"{dup_cases} (Dup), "
            f"{new_docket_count} (New)\n\n"
        )

        md = summary_header + current_md

    # ì´ì „ ë‚ ì§œ ì´ìŠˆ Close
    closed_nums = close_other_daily_issues(owner, repo, gh_token, issue_label, base_title, issue_title, issue_no, issue_url)
    if closed_nums:
        debug_log(f"ì´ì „ ë‚ ì§œ ì´ìŠˆ ìë™ Close: {closed_nums}")
    
    # KST ê¸°ì¤€ íƒ€ì„ìŠ¤íƒ¬í”„
    timestamp = datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M KST")

    comment_body = f"\n\n{md}"
    create_comment(owner, repo, gh_token, issue_no, comment_body)
    debug_log(f"Issue #{issue_no} ëŒ“ê¸€ ì—…ë¡œë“œ ì™„ë£Œ")

    # 5) Slack ìš”ì•½ ì „ì†¡
    # ============================================
    # Slack ì¶œë ¥ ê°œì„  (ìµœì¢… í¬ë§·)
    # ============================================

    slack_dedup_news = None
    slack_dedup_cases = None

    if "### ì¤‘ë³µ ì œê±° ìš”ì•½:" in md:
        m_news = re.search(
            r"â”” News (.+)",
            md,
        )

        m_cases = re.search(
            r"â”” Cases (.+)",
            md,
        )

        if m_news:
            slack_dedup_news = m_news.group(1).strip()

        if m_cases:
            slack_dedup_cases = m_cases.group(1).strip()



    slack_lines = []

    slack_lines.append("ğŸ“Š AI ì†Œì†¡ ëª¨ë‹ˆí„°ë§")
    slack_lines.append(f"ğŸ•’ {timestamp}")
    slack_lines.append("")

    # ğŸ” Dedup Summary
    if slack_dedup_news and slack_dedup_cases:
        slack_lines.append("ğŸ” Dedup Summary")
        slack_lines.append(f"â”” News {slack_dedup_news}")
        slack_lines.append(f"â”” Cases {slack_dedup_cases}")
        slack_lines.append("")

    # ğŸ“ˆ Collection Status
    slack_lines.append("ğŸ“ˆ Collection Status")
    slack_lines.append(f"â”” News: {len(lawsuits)}")
    slack_lines.append(
        f"â”” Cases: {docket_case_count} (Docs: {recap_doc_count})"
    )
    slack_lines.append("")

    # ğŸ”— GitHub
    slack_lines.append(f"ğŸ”— GitHub: <{issue_url}|#{issue_no}>")

    # ğŸ†• ìµœì‹  RECAP ë¬¸ì„œ
    if cl_docs:
        top = sorted(
            cl_docs,
            key=lambda x: getattr(x, "date_filed", ""),
            reverse=True,
        )[:3]

        slack_lines.append("")
        slack_lines.append("ğŸ†• ìµœì‹  RECAP ë¬¸ì„œ")

        for d in top:
            date = getattr(d, "date_filed", "N/A")
            name = getattr(d, "case_name", "Unknown Case")
            docket_id = getattr(d, "docket_id", None) 
            absolute_url = getattr(d, "absolute_url", None)

            if absolute_url:
                # ê°€ì¥ ì •í™•í•œ URL (slug í¬í•¨)
                docket_url = absolute_url
                if not docket_url.endswith("/"):
                    docket_url += "/"

                slack_lines.append(
                    f"â€¢ {date} | <{docket_url}|{name}>"
                )
            elif docket_id:
                # slug ìƒì„± (GitHub ì´ìŠˆì™€ ë™ì¼ êµ¬ì¡° ë§ì¶”ê¸°)
                # case_name â†’ slug ë³€í™˜
                slug = re.sub(r"[^a-zA-Z0-9]+", "-", name).strip("-").lower()

                docket_url = (
                    f"https://www.courtlistener.com/docket/"
                    f"{docket_id}/{slug}/"
                )

                slack_lines.append(
                    f"â€¢ {date} | <{docket_url}|{name}>"
                )
            else:
                slack_lines.append(f"â€¢ {date} | {name}")
    try:
        post_to_slack(slack_webhook, "\n".join(slack_lines))
        debug_log(f"Slack ì „ì†¡ ì™„ë£Œ")
    except Exception as e:
        debug_log(f"Slack ì „ì†¡ ì‹¤íŒ¨: {e}")
        
if __name__ == "__main__":
    main()
