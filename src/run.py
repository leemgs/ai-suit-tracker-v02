from __future__ import annotations
import os
from datetime import datetime, timezone
from zoneinfo import ZoneInfo

from .fetch import fetch_news
from .extract import load_known_cases, build_lawsuits_from_news
from .render import render_markdown
from .github_issue import find_or_create_issue, create_comment, close_other_daily_issues
from .github_issue import list_comments
from .slack import post_to_slack
from urllib.parse import urlparse, urlunparse
import re
from .courtlistener import (
    search_recent_documents,
    build_complaint_documents_from_hits,
    build_case_summaries_from_hits,
    build_case_summaries_from_docket_numbers,
    build_case_summaries_from_case_titles,
    build_documents_from_docket_ids,
)
from .queries import COURTLISTENER_QUERIES

def main() -> None:
    # 0) í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    owner = os.environ["GITHUB_OWNER"]
    repo = os.environ["GITHUB_REPO"]
    gh_token = os.environ["GITHUB_TOKEN"]
    slack_webhook = os.environ["SLACK_WEBHOOK_URL"]

    base_title = os.environ.get("ISSUE_TITLE_BASE", "AI ë¶ˆë²•/ë¬´ë‹¨ í•™ìŠµë°ì´í„° ì†Œì†¡ ëª¨ë‹ˆí„°ë§")
    lookback_days = int(os.environ.get("LOOKBACK_DAYS", "3"))
    # í•„ìš” ì‹œ 2ë¡œ ë³€ê²½: í™˜ê²½ë³€ìˆ˜ LOOKBACK_DAYS=2
    
    # KST ê¸°ì¤€ ë‚ ì§œ ìƒì„±
    now_kst = datetime.now(ZoneInfo("Asia/Seoul"))
    run_ts_kst = now_kst.strftime("%Y-%m-%d %H:%M")
    issue_day_kst = now_kst.strftime("%Y-%m-%d")
    issue_title = f"{base_title} ({issue_day_kst})"
    print(f"KST ê¸°ì¤€ ì‹¤í–‰ì‹œê°: {run_ts_kst}")
    
    issue_label = os.environ.get("ISSUE_LABEL", "ai-lawsuit-monitor")

    # 1) CourtListener ê²€ìƒ‰
    hits = []
    for q in COURTLISTENER_QUERIES:
        hits.extend(search_recent_documents(q, days=lookback_days, max_results=20))
    
    # ì¤‘ë³µ ì œê±°
    dedup = {}
    for h in hits:
        key = (h.get("absolute_url") or h.get("url") or "") + "|" + (h.get("caseName") or h.get("title") or "")
        dedup[key] = h
    hits = list(dedup.values())

    cl_docs = build_complaint_documents_from_hits(hits, days=lookback_days)
    # RECAP ë„ì¼“(ì‚¬ê±´) ìš”ì•½: "ë²•ì› ì‚¬ê±´(ë„ì¼“) í™•ì¸ ê±´ìˆ˜"ë¡œ ì‚¬ìš©
    cl_cases = build_case_summaries_from_hits(hits)

    # 2) ë‰´ìŠ¤ ìˆ˜ì§‘
    news = fetch_news()
    known = load_known_cases()
    lawsuits = build_lawsuits_from_news(news, known, lookback_days=lookback_days)

    # 2-1) ë‰´ìŠ¤ í…Œì´ë¸”ì˜ ì†Œì†¡ë²ˆí˜¸(ë„ì¼“ë²ˆí˜¸)ë¡œ RECAP ë„ì¼“/ë¬¸ì„œ í™•ì¥
    docket_numbers = [s.case_number for s in lawsuits if (s.case_number or "").strip() and s.case_number != "ë¯¸í™•ì¸"]
    extra_cases = build_case_summaries_from_docket_numbers(docket_numbers)

    # 2-2) ì†Œì†¡ë²ˆí˜¸ê°€ ì—†ë”ë¼ë„, 'ì†Œì†¡ì œëª©'(ì¶”ì • ì¼€ì´ìŠ¤ëª…)ìœ¼ë¡œ ë„ì¼“ í™•ì¥
    case_titles = [s.case_title for s in lawsuits if (s.case_title or "").strip() and s.case_title != "ë¯¸í™•ì¸"]
    extra_cases_by_title = build_case_summaries_from_case_titles(case_titles)

    merged_cases = {c.docket_id: c for c in (cl_cases + extra_cases + extra_cases_by_title)}
    cl_cases = list(merged_cases.values())

    # ë¬¸ì„œë„ docket id ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ ì‹œë„(Complaint ìš°ì„ , ì—†ìœ¼ë©´ fallback)
    docket_ids = list(merged_cases.keys())
    extra_docs = build_documents_from_docket_ids(docket_ids, days=lookback_days)
    merged_docs = {}
    for d in (cl_docs + extra_docs):
        key = (d.docket_id, d.doc_number, d.date_filed, d.document_url)
        merged_docs[key] = d
    cl_docs = list(merged_docs.values())

    docket_case_count = len(cl_cases)
    recap_doc_count = len(cl_docs)

    # 3) ë Œë”ë§
    md = render_markdown(lawsuits, cl_docs, cl_cases, lookback_days=lookback_days)
    md = f"### ì‹¤í–‰ ì‹œê°(KST): {run_ts_kst}\n\n" + md
   

    # ==========================================
    # ğŸ”¹ ë…¸ì´ì¦ˆ ì œê±° + URL Canonicalization
    # ==========================================

    def normalize_url(url: str) -> str:
        try:
            parsed = urlparse(url)
            return urlunparse((parsed.scheme, parsed.netloc, parsed.path, "", "", ""))
        except Exception:
            return url

    cleaned_lines = []
    seen_urls = set()

    for line in md.splitlines():
        stripped = line.strip()

        # Google RSS ì œê±°
        if "news.google.com/rss" in stripped:
            continue
        # bullet URL ì²˜ë¦¬
        if stripped.startswith("- http"):
            raw_url = stripped[2:].strip()
            normalized = normalize_url(raw_url)

            # CourtListener ì™¸ URL ì œê±° (RECAP ë³´í˜¸)
            if "courtlistener.com" not in normalized:
                continue

            if normalized in seen_urls:
                continue

            seen_urls.add(normalized)
            cleaned_lines.append(f"- {normalized}")
        else:
            cleaned_lines.append(line)

    md = "\n".join(cleaned_lines)   
    
    print("===== REPORT BEGIN =====")
    print(md[:1000]) # ë¡œê·¸ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¦¬ë¯€ë¡œ ì¼ë¶€ë§Œ ì¶œë ¥
    print("===== REPORT END =====")

    # 4) GitHub Issue ì‘ì—…
    issue_no = find_or_create_issue(owner, repo, gh_token, issue_title, issue_label)
    issue_url = f"https://github.com/{owner}/{repo}/issues/{issue_no}"
    
    # ì´ì „ ë‚ ì§œ ì´ìŠˆ Close
    closed_nums = close_other_daily_issues(owner, repo, gh_token, issue_label, base_title, issue_title, issue_no, issue_url)
    if closed_nums:
        print(f"ì´ì „ ë‚ ì§œ ì´ìŠˆ ìë™ Close: {closed_nums}")
    
    # KST ê¸°ì¤€ íƒ€ì„ìŠ¤íƒ¬í”„
    timestamp = datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M KST")

    comments = list_comments(owner, repo, gh_token, issue_no)

    # ìµœì´ˆ ìƒì„± (ëŒ“ê¸€ì´ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš°)
    if not comments:
        create_comment(owner, repo, gh_token, issue_no, md)
        print(f"Issue #{issue_no} ìµœì´ˆ ì „ì²´ ë¦¬í¬íŠ¸ ë“±ë¡ ì™„ë£Œ")
    else:
        last_comment_body = comments[-1].get("body", "")

        def split_sections(text):
            sections = {}
            current = None
            buffer = []
            for line in text.splitlines():
                if line.startswith("## "):
                    if current:
                        sections[current] = "\n".join(buffer)
                    current = line.strip()
                    buffer = []
                else:
                    buffer.append(line)
            if current:
                sections[current] = "\n".join(buffer)
            return sections

        def extract_bullets(section_text):
            bullets = set()
            for line in section_text.splitlines():
                stripped = line.strip()
                # ğŸ”¹ ì‹¤ì œ ë°ì´í„° í•­ëª©ë§Œ í—ˆìš© (URL í¬í•¨ í•­ëª©ë§Œ)
                if stripped.startswith("- ") and "http" in stripped:
                    raw_url = stripped.split("http", 1)[1]
                    raw_url = "http" + raw_url
                    parsed = urlparse(raw_url)
                    normalized = urlunparse((parsed.scheme, parsed.netloc, parsed.path, "", "", ""))
                    bullets.add(f"- {normalized}")
            return bullets

        sections_now = split_sections(md)
        sections_prev = split_sections(last_comment_body)

        output_lines = []
        total_new = 0

        output_lines.append(f"### ì‹¤í–‰ ì‹œê°(KST): {timestamp}")
        output_lines.append("")

        for title, content_now in sections_now.items():
            output_lines.append(title)

            bullets_now = extract_bullets(content_now)
            bullets_prev = extract_bullets(sections_prev.get(title, ""))

            new_items = sorted(bullets_now - bullets_prev)

            if new_items:
                total_new += len(new_items)
                output_lines.extend(new_items)
            else:
                output_lines.append("- ìƒˆë¡­ê²Œ ì¶”ê°€ëœ ì •ë³´ ì—†ìŒ.")

            output_lines.append("")

        if total_new == 0:
            create_comment(owner, repo, gh_token, issue_no, "ìƒˆë¡­ê²Œ ì¶”ê°€ëœ ì •ë³´ê°€ ì—†ìŠ´ë‹ˆë‹¤.")
            print("ì‹ ê·œ ë°ì´í„° ì—†ìŒ")
        else:
            create_comment(owner, repo, gh_token, issue_no, "\n".join(output_lines))
            print("ì‹ ê·œ í•­ëª©ë§Œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")

    # 5) Slack ìš”ì•½ ì „ì†¡
    summary_lines = [
        f"*AI ì†Œì†¡ ëª¨ë‹ˆí„°ë§ ì—…ë°ì´íŠ¸* ({timestamp})",
        f"- ì–¸ë¡ ë³´ë„ ê¸°ë°˜ ìˆ˜ì§‘ ê±´ìˆ˜: {len(lawsuits)}ê±´",
        f"- ë²•ì› ì‚¬ê±´(RECAP ë„ì¼“) í™•ì¸ ê±´ìˆ˜: {docket_case_count}ê±´",
        f"- ë²•ì› ë¬¸ì„œ(RECAP Complaint ë“±) í™•ë³´ ê±´ìˆ˜: {recap_doc_count}ê±´",
        f"- GitHub Issue (For more details): <{issue_url}|#{issue_no}>",
    ]
    
    if cl_docs:
        # date_filed ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        top = sorted(cl_docs, key=lambda x: getattr(x, 'date_filed', ''), reverse=True)[:3]
        summary_lines.append("- ìµœì‹  RECAP ë¬¸ì„œ:")
        for d in top:
            date = getattr(d, 'date_filed', 'N/A')
            name = getattr(d, 'case_name', 'Unknown Case')
            summary_lines.append(f"  â€¢ {date} | {name}")
    
    post_to_slack(slack_webhook, "\n".join(summary_lines))
    print("Slack ì „ì†¡ ì™„ë£Œ")

if __name__ == "__main__":
    main()

